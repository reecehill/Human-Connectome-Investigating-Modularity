"""
This type stub file was generated by pyright.
"""

from .. import config
from .base import BaseInterface, BaseInterfaceInputSpec, DynamicTraitedSpec, File, InputMultiPath, LibraryBaseInterface, OutputMultiPath, SimpleInterface, Str, TraitedSpec, traits

""" Set of interfaces that allow interaction with data. Currently
    available interfaces are:

    DataSource: Generic nifti to named Nifti interface
    DataSink: Generic named output from interfaces to data store
    XNATSource: preliminary interface to XNAT

    To come :
    XNATSink
"""
iflogger = ...
def copytree(src, dst, use_hardlink=...): # -> None:
    """Recursively copy a directory tree using
    nipype.utils.filemanip.copyfile()

    This is not a thread-safe routine. However, in the case of creating new
    directories, it checks to see if a particular directory has already been
    created by another process.
    """
    ...

def add_traits(base, names, trait_type=...):
    """Add traits to a traited class.

    All traits are set to Undefined by default
    """
    ...

class IOBase(BaseInterface):
    ...


class ProgressPercentage:
    """
    Callable class instsance (via __call__ method) that displays
    upload percentage of a file to S3
    """
    def __init__(self, filename) -> None:
        """ """
        ...
    
    def __call__(self, bytes_amount): # -> None:
        """ """
        ...
    


class DataSinkInputSpec(DynamicTraitedSpec, BaseInterfaceInputSpec):
    """ """
    base_directory = ...
    container = ...
    parameterization = ...
    strip_dir = ...
    substitutions = InputMultiPath(traits.Tuple(Str, Str), desc="List of 2-tuples reflecting string " "to substitute and string to replace " "it with")
    regexp_substitutions = InputMultiPath(traits.Tuple(Str, Str), desc="List of 2-tuples reflecting a pair of a " "Python regexp pattern and a replacement " "string. Invoked after string `substitutions`")
    _outputs = ...
    remove_dest_dir = ...
    creds_path = ...
    encrypt_bucket_keys = ...
    bucket = ...
    local_copy = ...
    def __setattr__(self, key, value): # -> None:
        ...
    


class DataSinkOutputSpec(TraitedSpec):
    out_file = ...


class DataSink(IOBase):
    """
    Generic datasink module to store structured outputs.

    Primarily for use within a workflow. This interface allows arbitrary
    creation of input attributes. The names of these attributes define the
    directory structure to create for storage of the files or directories.

    The attributes take the following form::

      string[[.[@]]string[[.[@]]string]] ...

    where parts between ``[]`` are optional.

    An attribute such as contrasts.@con will create a 'contrasts' directory
    to store the results linked to the attribute. If the ``@`` is left out, such
    as in 'contrasts.con', a subdirectory 'con' will be created under
    'contrasts'.

    The general form of the output is::

       'base_directory/container/parameterization/destloc/filename'

    ``destloc = string[[.[@]]string[[.[@]]string]]`` and
    ``filename`` come from the input to the connect statement.

    .. warning::

        This is not a thread-safe node because it can write to a common
        shared location. It will not complain when it overwrites a file.

    .. note::

        If both substitutions and regexp_substitutions are used, then
        substitutions are applied first followed by regexp_substitutions.

        This interface **cannot** be used in a MapNode as the inputs are
        defined only when the connect statement is executed.

    Examples
    --------
    >>> ds = DataSink()
    >>> ds.inputs.base_directory = 'results_dir'
    >>> ds.inputs.container = 'subject'
    >>> ds.inputs.structural = 'structural.nii'
    >>> setattr(ds.inputs, 'contrasts.@con', ['cont1.nii', 'cont2.nii'])
    >>> setattr(ds.inputs, 'contrasts.alt', ['cont1a.nii', 'cont2a.nii'])
    >>> ds.run()  # doctest: +SKIP

    To use DataSink in a MapNode, its inputs have to be defined at the
    time the interface is created.

    >>> ds = DataSink(infields=['contasts.@con'])
    >>> ds.inputs.base_directory = 'results_dir'
    >>> ds.inputs.container = 'subject'
    >>> ds.inputs.structural = 'structural.nii'
    >>> setattr(ds.inputs, 'contrasts.@con', ['cont1.nii', 'cont2.nii'])
    >>> setattr(ds.inputs, 'contrasts.alt', ['cont1a.nii', 'cont2a.nii'])
    >>> ds.run()  # doctest: +SKIP

    """
    input_spec = DataSinkInputSpec
    output_spec = DataSinkOutputSpec
    def __init__(self, infields=..., force_run=..., **kwargs) -> None:
        """
        Parameters
        ----------
        infields : list of str
            Indicates the input fields to be dynamically created
        """
        ...
    


class S3DataGrabberInputSpec(DynamicTraitedSpec, BaseInterfaceInputSpec):
    anon = ...
    region = ...
    bucket = ...
    bucket_path = ...
    local_directory = ...
    raise_on_empty = ...
    sort_filelist = ...
    template = ...
    template_args = ...


class S3DataGrabber(LibraryBaseInterface, IOBase):
    """
    Pull data from an Amazon S3 Bucket.

    Generic datagrabber module that wraps around glob in an
    intelligent way for neuroimaging tasks to grab files from
    Amazon S3

    Works exactly like DataGrabber, except, you must specify an
    S3 "bucket" and "bucket_path" to search for your data and a
    "local_directory" to store the data. "local_directory"
    should be a location on HDFS for Spark jobs. Additionally,
    "template" uses regex style formatting, rather than the
    glob-style found in the original DataGrabber.

    Examples
    --------
    >>> s3grab = S3DataGrabber(infields=['subj_id'], outfields=["func", "anat"])
    >>> s3grab.inputs.bucket = 'openneuro'
    >>> s3grab.inputs.sort_filelist = True
    >>> s3grab.inputs.template = '*'
    >>> s3grab.inputs.anon = True
    >>> s3grab.inputs.bucket_path = 'ds000101/ds000101_R2.0.0/uncompressed/'
    >>> s3grab.inputs.local_directory = '/tmp'
    >>> s3grab.inputs.field_template = {'anat': '%s/anat/%s_T1w.nii.gz',
    ...                                 'func': '%s/func/%s_task-simon_run-1_bold.nii.gz'}
    >>> s3grab.inputs.template_args = {'anat': [['subj_id', 'subj_id']],
    ...                                'func': [['subj_id', 'subj_id']]}
    >>> s3grab.inputs.subj_id = 'sub-01'
    >>> s3grab.run()  # doctest: +SKIP

    """
    input_spec = S3DataGrabberInputSpec
    output_spec = DynamicTraitedSpec
    _always_run = ...
    _pkg = ...
    def __init__(self, infields=..., outfields=..., **kwargs) -> None:
        """
        Parameters
        ----------
        infields : list of str
            Indicates the input fields to be dynamically created

        outfields: list of str
            Indicates output fields to be dynamically created

        See class examples for usage

        """
        ...
    
    def s3tolocal(self, s3path, bkt):
        ...
    


class DataGrabberInputSpec(DynamicTraitedSpec, BaseInterfaceInputSpec):
    base_directory = ...
    raise_on_empty = ...
    drop_blank_outputs = ...
    sort_filelist = ...
    template = ...
    template_args = ...


class DataGrabber(IOBase):
    """
    Find files on a filesystem.

    Generic datagrabber module that wraps around glob in an
    intelligent way for neuroimaging tasks to grab files

    .. important::

       Doesn't support directories currently

    Examples
    --------
    >>> from nipype.interfaces.io import DataGrabber

    Pick all files from current directory

    >>> dg = DataGrabber()
    >>> dg.inputs.template = '*'

    Pick file foo/foo.nii from current directory

    >>> dg.inputs.template = '%s/%s.dcm'
    >>> dg.inputs.template_args['outfiles']=[['dicomdir','123456-1-1.dcm']]

    Same thing but with dynamically created fields

    >>> dg = DataGrabber(infields=['arg1','arg2'])
    >>> dg.inputs.template = '%s/%s.nii'
    >>> dg.inputs.arg1 = 'foo'
    >>> dg.inputs.arg2 = 'foo'

    however this latter form can be used with iterables and iterfield in a
    pipeline.

    Dynamically created, user-defined input and output fields

    >>> dg = DataGrabber(infields=['sid'], outfields=['func','struct','ref'])
    >>> dg.inputs.base_directory = '.'
    >>> dg.inputs.template = '%s/%s.nii'
    >>> dg.inputs.template_args['func'] = [['sid',['f3','f5']]]
    >>> dg.inputs.template_args['struct'] = [['sid',['struct']]]
    >>> dg.inputs.template_args['ref'] = [['sid','ref']]
    >>> dg.inputs.sid = 's1'

    Change the template only for output field struct. The rest use the
    general template

    >>> dg.inputs.field_template = dict(struct='%s/struct.nii')
    >>> dg.inputs.template_args['struct'] = [['sid']]

    """
    input_spec = DataGrabberInputSpec
    output_spec = DynamicTraitedSpec
    _always_run = ...
    def __init__(self, infields=..., outfields=..., **kwargs) -> None:
        """
        Parameters
        ----------
        infields : list of str
            Indicates the input fields to be dynamically created

        outfields: list of str
            Indicates output fields to be dynamically created

        See class examples for usage

        """
        ...
    


class SelectFilesInputSpec(DynamicTraitedSpec, BaseInterfaceInputSpec):
    base_directory = ...
    sort_filelist = ...
    raise_on_empty = ...
    force_lists = ...


class SelectFiles(IOBase):
    """
    Flexibly collect data from disk to feed into workflows.

    This interface uses Python's {}-based string formatting syntax to plug
    values (possibly known only at workflow execution time) into string
    templates and collect files from persistent storage. These templates can
    also be combined with glob wildcards (``*``, ``?``) and character ranges (``[...]``).
    The field names in the formatting template (i.e. the terms in braces) will
    become inputs fields on the interface, and the keys in the templates
    dictionary will form the output fields.

    Examples
    --------
    >>> import pprint
    >>> from nipype import SelectFiles, Node
    >>> templates={"T1": "{subject_id}/struct/T1.nii",
    ...            "epi": "{subject_id}/func/f[0,1].nii"}
    >>> dg = Node(SelectFiles(templates), "selectfiles")
    >>> dg.inputs.subject_id = "subj1"
    >>> pprint.pprint(dg.outputs.get())  # doctest:
    {'T1': <undefined>, 'epi': <undefined>}

    Note that SelectFiles does not support lists as inputs for the dynamic
    fields. Attempts to do so may lead to unexpected results because brackets
    also express glob character ranges. For example,

    >>> templates["epi"] = "{subject_id}/func/f{run}.nii"
    >>> dg = Node(SelectFiles(templates), "selectfiles")
    >>> dg.inputs.subject_id = "subj1"
    >>> dg.inputs.run = [10, 11]

    would match f0.nii or f1.nii, not f10.nii or f11.nii.

    """
    input_spec = SelectFilesInputSpec
    output_spec = DynamicTraitedSpec
    _always_run = ...
    def __init__(self, templates, **kwargs) -> None:
        """Create an instance with specific input fields.

        Parameters
        ----------
        templates : dictionary
            Mapping from string keys to string template values.
            The keys become output fields on the interface.
            The templates should use {}-formatting syntax, where
            the names in curly braces become inputs fields on the interface.
            Format strings can also use glob wildcards to match multiple
            files. At runtime, the values of the interface inputs will be
            plugged into these templates, and the resulting strings will be
            used to select files.

        """
        ...
    


class DataFinderInputSpec(DynamicTraitedSpec, BaseInterfaceInputSpec):
    root_paths = ...
    match_regex = ...
    ignore_regexes = ...
    max_depth = ...
    min_depth = ...
    unpack_single = ...


class DataFinder(IOBase):
    r"""Search for paths that match a given regular expression. Allows a less
    proscriptive approach to gathering input files compared to DataGrabber.
    Will recursively search any subdirectories by default. This can be limited
    with the min/max depth options.
    Matched paths are available in the output 'out_paths'. Any named groups of
    captured text from the regular expression are also available as outputs of
    the same name.

    Examples
    --------
    >>> from nipype.interfaces.io import DataFinder
    >>> df = DataFinder()
    >>> df.inputs.root_paths = '.'
    >>> df.inputs.match_regex = r'.+/(?P<series_dir>.+(qT1|ep2d_fid_T1).+)/(?P<basename>.+)\.nii.gz'
    >>> result = df.run() # doctest: +SKIP
    >>> result.outputs.out_paths  # doctest: +SKIP
    ['./027-ep2d_fid_T1_Gd4/acquisition.nii.gz',
     './018-ep2d_fid_T1_Gd2/acquisition.nii.gz',
     './016-ep2d_fid_T1_Gd1/acquisition.nii.gz',
     './013-ep2d_fid_T1_pre/acquisition.nii.gz']
    >>> result.outputs.series_dir  # doctest: +SKIP
    ['027-ep2d_fid_T1_Gd4',
     '018-ep2d_fid_T1_Gd2',
     '016-ep2d_fid_T1_Gd1',
     '013-ep2d_fid_T1_pre']
    >>> result.outputs.basename  # doctest: +SKIP
    ['acquisition',
     'acquisition'
     'acquisition',
     'acquisition']

    """
    input_spec = DataFinderInputSpec
    output_spec = DynamicTraitedSpec
    _always_run = ...


class FSSourceInputSpec(BaseInterfaceInputSpec):
    subjects_dir = ...
    subject_id = ...
    hemi = ...


class FSSourceOutputSpec(TraitedSpec):
    T1 = ...
    aseg = ...
    brain = ...
    brainmask = ...
    filled = ...
    norm = ...
    nu = ...
    orig = ...
    rawavg = ...
    ribbon = OutputMultiPath(File(exists=True), desc="Volumetric maps of cortical ribbons", loc="mri", altkey="*ribbon")
    wm = ...
    wmparc = ...
    curv = OutputMultiPath(File(exists=True), desc="Maps of surface curvature", loc="surf")
    avg_curv = OutputMultiPath(File(exists=True), desc="Average atlas curvature, sampled to subject", loc="surf")
    inflated = OutputMultiPath(File(exists=True), desc="Inflated surface meshes", loc="surf")
    pial = OutputMultiPath(File(exists=True), desc="Gray matter/pia mater surface meshes", loc="surf")
    area_pial = OutputMultiPath(File(exists=True), desc="Mean area of triangles each vertex on the pial surface is " "associated with", loc="surf", altkey="area.pial")
    curv_pial = OutputMultiPath(File(exists=True), desc="Curvature of pial surface", loc="surf", altkey="curv.pial")
    smoothwm = OutputMultiPath(File(exists=True), loc="surf", desc="Smoothed original surface meshes")
    sphere = OutputMultiPath(File(exists=True), desc="Spherical surface meshes", loc="surf")
    sulc = OutputMultiPath(File(exists=True), desc="Surface maps of sulcal depth", loc="surf")
    thickness = OutputMultiPath(File(exists=True), loc="surf", desc="Surface maps of cortical thickness")
    volume = OutputMultiPath(File(exists=True), desc="Surface maps of cortical volume", loc="surf")
    white = OutputMultiPath(File(exists=True), desc="White/gray matter surface meshes", loc="surf")
    jacobian_white = OutputMultiPath(File(exists=True), desc="Distortion required to register to spherical atlas", loc="surf")
    graymid = OutputMultiPath(File(exists=True), desc="Graymid/midthickness surface meshes", loc="surf", altkey=["graymid", "midthickness"])
    label = OutputMultiPath(File(exists=True), desc="Volume and surface label files", loc="label", altkey="*label")
    annot = OutputMultiPath(File(exists=True), desc="Surface annotation files", loc="label", altkey="*annot")
    aparc_aseg = OutputMultiPath(File(exists=True), loc="mri", altkey="aparc*aseg", desc="Aparc parcellation projected into aseg volume")
    sphere_reg = OutputMultiPath(File(exists=True), loc="surf", altkey="sphere.reg", desc="Spherical registration file")
    aseg_stats = OutputMultiPath(File(exists=True), loc="stats", altkey="aseg", desc="Automated segmentation statistics file")
    wmparc_stats = OutputMultiPath(File(exists=True), loc="stats", altkey="wmparc", desc="White matter parcellation statistics file")
    aparc_stats = OutputMultiPath(File(exists=True), loc="stats", altkey="aparc", desc="Aparc parcellation statistics files")
    BA_stats = OutputMultiPath(File(exists=True), loc="stats", altkey="BA", desc="Brodmann Area statistics files")
    aparc_a2009s_stats = OutputMultiPath(File(exists=True), loc="stats", altkey="aparc.a2009s", desc="Aparc a2009s parcellation statistics files")
    curv_stats = OutputMultiPath(File(exists=True), loc="stats", altkey="curv", desc="Curvature statistics files")
    entorhinal_exvivo_stats = OutputMultiPath(File(exists=True), loc="stats", altkey="entorhinal_exvivo", desc="Entorhinal exvivo statistics files")


class FreeSurferSource(IOBase):
    """Generates freesurfer subject info from their directories.

    Examples
    --------
    >>> from nipype.interfaces.io import FreeSurferSource
    >>> fs = FreeSurferSource()
    >>> #fs.inputs.subjects_dir = '.'
    >>> fs.inputs.subject_id = 'PWS04'
    >>> res = fs.run() # doctest: +SKIP

    >>> fs.inputs.hemi = 'lh'
    >>> res = fs.run() # doctest: +SKIP

    """
    input_spec = FSSourceInputSpec
    output_spec = FSSourceOutputSpec
    _always_run = ...
    _additional_metadata = ...


class XNATSourceInputSpec(DynamicTraitedSpec, BaseInterfaceInputSpec):
    query_template = ...
    query_template_args = ...
    server = ...
    user = ...
    pwd = ...
    config = ...
    cache_dir = ...


class XNATSource(LibraryBaseInterface, IOBase):
    """
    Pull data from an XNAT server.

    Generic XNATSource module that wraps around the pyxnat module in
    an intelligent way for neuroimaging tasks to grab files and data
    from an XNAT server.

    Examples
    --------
    Pick all files from current directory

    >>> dg = XNATSource()
    >>> dg.inputs.template = '*'

    >>> dg = XNATSource(infields=['project','subject','experiment','assessor','inout'])
    >>> dg.inputs.query_template = '/projects/%s/subjects/%s/experiments/%s' \
               '/assessors/%s/%s_resources/files'
    >>> dg.inputs.project = 'IMAGEN'
    >>> dg.inputs.subject = 'IMAGEN_000000001274'
    >>> dg.inputs.experiment = '*SessionA*'
    >>> dg.inputs.assessor = '*ADNI_MPRAGE_nii'
    >>> dg.inputs.inout = 'out'

    >>> dg = XNATSource(infields=['sid'],outfields=['struct','func'])
    >>> dg.inputs.query_template = '/projects/IMAGEN/subjects/%s/experiments/*SessionA*' \
               '/assessors/*%s_nii/out_resources/files'
    >>> dg.inputs.query_template_args['struct'] = [['sid','ADNI_MPRAGE']]
    >>> dg.inputs.query_template_args['func'] = [['sid','EPI_faces']]
    >>> dg.inputs.sid = 'IMAGEN_000000001274'

    """
    input_spec = XNATSourceInputSpec
    output_spec = DynamicTraitedSpec
    _pkg = ...
    def __init__(self, infields=..., outfields=..., **kwargs) -> None:
        """
        Parameters
        ----------
        infields : list of str
            Indicates the input fields to be dynamically created

        outfields: list of str
            Indicates output fields to be dynamically created

        See class examples for usage

        """
        ...
    


class XNATSinkInputSpec(DynamicTraitedSpec, BaseInterfaceInputSpec):
    _outputs = ...
    server = ...
    user = ...
    pwd = ...
    config = ...
    cache_dir = ...
    project_id = ...
    subject_id = ...
    experiment_id = ...
    assessor_id = ...
    reconstruction_id = ...
    share = ...
    def __setattr__(self, key, value): # -> None:
        ...
    


class XNATSink(LibraryBaseInterface, IOBase):
    """Generic datasink module that takes a directory containing a
    list of nifti files and provides a set of structured output
    fields.
    """
    input_spec = XNATSinkInputSpec
    _pkg = ...


def quote_id(string): # -> str:
    ...

def unquote_id(string): # -> str:
    ...

def push_file(self, xnat, file_name, out_key, uri_template_args): # -> None:
    ...

def capture_provenance(): # -> None:
    ...

def push_provenance(): # -> None:
    ...

class SQLiteSinkInputSpec(DynamicTraitedSpec, BaseInterfaceInputSpec):
    database_file = ...
    table_name = ...


class SQLiteSink(LibraryBaseInterface, IOBase):
    """
    Very simple frontend for storing values into SQLite database.

    .. warning::

        This is not a thread-safe node because it can write to a common
        shared location. It will not complain when it overwrites a file.

    Examples
    --------

    >>> sql = SQLiteSink(input_names=['subject_id', 'some_measurement'])
    >>> sql.inputs.database_file = 'my_database.db'
    >>> sql.inputs.table_name = 'experiment_results'
    >>> sql.inputs.subject_id = 's1'
    >>> sql.inputs.some_measurement = 11.4
    >>> sql.run() # doctest: +SKIP

    """
    input_spec = SQLiteSinkInputSpec
    _pkg = ...
    def __init__(self, input_names, **inputs) -> None:
        ...
    


class MySQLSinkInputSpec(DynamicTraitedSpec, BaseInterfaceInputSpec):
    host = ...
    config = ...
    database_name = ...
    table_name = ...
    username = ...
    password = ...


class MySQLSink(IOBase):
    """
    Very simple frontend for storing values into MySQL database.

    Examples
    --------

    >>> sql = MySQLSink(input_names=['subject_id', 'some_measurement'])
    >>> sql.inputs.database_name = 'my_database'
    >>> sql.inputs.table_name = 'experiment_results'
    >>> sql.inputs.username = 'root'
    >>> sql.inputs.password = 'secret'
    >>> sql.inputs.subject_id = 's1'
    >>> sql.inputs.some_measurement = 11.4
    >>> sql.run() # doctest: +SKIP

    """
    input_spec = MySQLSinkInputSpec
    def __init__(self, input_names, **inputs) -> None:
        ...
    


class SSHDataGrabberInputSpec(DataGrabberInputSpec):
    hostname = ...
    username = ...
    password = ...
    download_files = ...
    base_directory = ...
    template_expression = ...
    ssh_log_to_file = ...


class SSHDataGrabber(LibraryBaseInterface, DataGrabber):
    """
    Extension of DataGrabber module that downloads the file list and
    optionally the files from a SSH server. The SSH operation must
    not need user and password so an SSH agent must be active in
    where this module is being run.


    .. attention::

       Doesn't support directories currently

    Examples
    --------
    >>> from nipype.interfaces.io import SSHDataGrabber
    >>> dg = SSHDataGrabber()
    >>> dg.inputs.hostname = 'test.rebex.net'
    >>> dg.inputs.user = 'demo'
    >>> dg.inputs.password = 'password'
    >>> dg.inputs.base_directory = 'pub/example'

    Pick all files from the base directory

    >>> dg.inputs.template = '*'

    Pick all files starting with "s" and a number from current directory

    >>> dg.inputs.template_expression = 'regexp'
    >>> dg.inputs.template = 'pop[0-9].*'

    Same thing but with dynamically created fields

    >>> dg = SSHDataGrabber(infields=['arg1','arg2'])
    >>> dg.inputs.hostname = 'test.rebex.net'
    >>> dg.inputs.user = 'demo'
    >>> dg.inputs.password = 'password'
    >>> dg.inputs.base_directory = 'pub'
    >>> dg.inputs.template = '%s/%s.txt'
    >>> dg.inputs.arg1 = 'example'
    >>> dg.inputs.arg2 = 'foo'

    however this latter form can be used with iterables and iterfield in a
    pipeline.

    Dynamically created, user-defined input and output fields

    >>> dg = SSHDataGrabber(infields=['sid'], outfields=['func','struct','ref'])
    >>> dg.inputs.hostname = 'myhost.com'
    >>> dg.inputs.base_directory = '/main_folder/my_remote_dir'
    >>> dg.inputs.template_args['func'] = [['sid',['f3','f5']]]
    >>> dg.inputs.template_args['struct'] = [['sid',['struct']]]
    >>> dg.inputs.template_args['ref'] = [['sid','ref']]
    >>> dg.inputs.sid = 's1'

    Change the template only for output field struct. The rest use the
    general template

    >>> dg.inputs.field_template = dict(struct='%s/struct.nii')
    >>> dg.inputs.template_args['struct'] = [['sid']]

    """
    input_spec = SSHDataGrabberInputSpec
    output_spec = DynamicTraitedSpec
    _always_run = ...
    _pkg = ...
    def __init__(self, infields=..., outfields=..., **kwargs) -> None:
        """
        Parameters
        ----------
        infields : list of str
            Indicates the input fields to be dynamically created

        outfields: list of str
            Indicates output fields to be dynamically created

        See class examples for usage

        """
        ...
    


class JSONFileGrabberInputSpec(DynamicTraitedSpec, BaseInterfaceInputSpec):
    in_file = ...
    defaults = ...


class JSONFileGrabber(IOBase):
    """
    Datagrabber interface that loads a json file and generates an output for
    every first-level object

    Example
    -------

    >>> import pprint
    >>> from nipype.interfaces.io import JSONFileGrabber
    >>> jsonSource = JSONFileGrabber()
    >>> jsonSource.inputs.defaults = {'param1': 'overrideMe', 'param3': 1.0}
    >>> res = jsonSource.run()
    >>> pprint.pprint(res.outputs.get())
    {'param1': 'overrideMe', 'param3': 1.0}
    >>> jsonSource.inputs.in_file = os.path.join(datadir, 'jsongrabber.txt')
    >>> res = jsonSource.run()
    >>> pprint.pprint(res.outputs.get())  # doctest:, +ELLIPSIS
    {'param1': 'exampleStr', 'param2': 4, 'param3': 1.0}
    """
    input_spec = JSONFileGrabberInputSpec
    output_spec = DynamicTraitedSpec
    _always_run = ...


class JSONFileSinkInputSpec(DynamicTraitedSpec, BaseInterfaceInputSpec):
    out_file = ...
    in_dict = ...
    _outputs = ...
    def __setattr__(self, key, value): # -> None:
        ...
    


class JSONFileSinkOutputSpec(TraitedSpec):
    out_file = ...


class JSONFileSink(IOBase):
    """
    Very simple frontend for storing values into a JSON file.
    Entries already existing in in_dict will be overridden by matching
    entries dynamically added as inputs.

    .. warning::

        This is not a thread-safe node because it can write to a common
        shared location. It will not complain when it overwrites a file.

    Examples
    --------
    >>> jsonsink = JSONFileSink(input_names=['subject_id',
    ...                         'some_measurement'])
    >>> jsonsink.inputs.subject_id = 's1'
    >>> jsonsink.inputs.some_measurement = 11.4
    >>> jsonsink.run() # doctest: +SKIP

    Using a dictionary as input:

    >>> dictsink = JSONFileSink()
    >>> dictsink.inputs.in_dict = {'subject_id': 's1',
    ...                            'some_measurement': 11.4}
    >>> dictsink.run() # doctest: +SKIP

    """
    input_spec = JSONFileSinkInputSpec
    output_spec = JSONFileSinkOutputSpec
    def __init__(self, infields=..., force_run=..., **inputs) -> None:
        ...
    


class BIDSDataGrabberInputSpec(DynamicTraitedSpec):
    base_dir = ...
    output_query = ...
    load_layout = ...
    raise_on_empty = ...
    index_derivatives = ...
    extra_derivatives = ...


class BIDSDataGrabber(LibraryBaseInterface, IOBase):
    """BIDS datagrabber module that wraps around pybids to allow arbitrary
    querying of BIDS datasets.

    Examples
    --------

    .. setup::

        >>> try:
        ...     import bids
        ... except ImportError:
        ...     pytest.skip()

    By default, the BIDSDataGrabber fetches anatomical and functional images
    from a project, and makes BIDS entities (e.g. subject) available for
    filtering outputs.

    >>> bg = BIDSDataGrabber()
    >>> bg.inputs.base_dir = 'ds005/'
    >>> bg.inputs.subject = '01'
    >>> results = bg.run() # doctest: +SKIP


    Dynamically created, user-defined output fields can also be defined to
    return different types of outputs from the same project. All outputs
    are filtered on common entities, which can be explicitly defined as
    infields.

    >>> bg = BIDSDataGrabber(infields = ['subject'])
    >>> bg.inputs.base_dir = 'ds005/'
    >>> bg.inputs.subject = '01'
    >>> bg.inputs.output_query['dwi'] = dict(datatype='dwi')
    >>> results = bg.run() # doctest: +SKIP

    """
    input_spec = BIDSDataGrabberInputSpec
    output_spec = DynamicTraitedSpec
    _always_run = ...
    _pkg = ...
    def __init__(self, infields=..., **kwargs) -> None:
        """
        Parameters
        ----------
        infields : list of str
            Indicates the input fields to be dynamically created
        """
        ...
    


class ExportFileInputSpec(BaseInterfaceInputSpec):
    in_file = ...
    out_file = ...
    check_extension = ...
    clobber = ...


class ExportFileOutputSpec(TraitedSpec):
    out_file = ...


class ExportFile(SimpleInterface):
    """Export a file to an absolute path.

    This interface copies an input file to a named output file.
    This is useful to save individual files to a specific location,
    instead of more flexible interfaces like DataSink.

    Examples
    --------
    >>> from nipype.interfaces.io import ExportFile
    >>> import os.path as op
    >>> ef = ExportFile()
    >>> ef.inputs.in_file = "T1.nii.gz"
    >>> os.mkdir("output_folder")
    >>> ef.inputs.out_file = op.abspath("output_folder/sub1_out.nii.gz")
    >>> res = ef.run()
    >>> os.path.exists(res.outputs.out_file)
    True

    """
    input_spec = ExportFileInputSpec
    output_spec = ExportFileOutputSpec


