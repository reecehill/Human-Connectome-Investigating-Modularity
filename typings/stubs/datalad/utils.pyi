"""
This type stub file was generated by pyright.
"""

import inspect
import logging
import string
import threading
import time
from collections.abc import Callable, Iterable, Iterator, Sequence
from contextlib import contextmanager
from functools import lru_cache
from pathlib import Path
from types import ModuleType, TracebackType
from typing import Any, Dict, IO, List, NamedTuple, Optional, TextIO, Tuple, TypeVar, Union, overload
from datalad.typing import K, Literal, P, T, V

lgr = ...
platform_system = ...
on_windows = ...
on_osx = ...
on_linux = ...
@lru_cache()
def get_linux_distribution() -> tuple[str, str, str]:
    """Compatibility wrapper for {platform,distro}.linux_distribution().
    """
    ...

linux_distribution_release = ...
CMD_MAX_ARG_HARDCODED = ...
CMD_MAX_ARG = ...
if CMD_MAX_ARG > CMD_MAX_ARG_HARDCODED * 1000000:
    CMD_MAX_ARG = ...
CMD_MAX_ARG = ...
class ArgSpecFake(NamedTuple):
    args: list[str]
    varargs: Optional[str]
    keywords: Optional[str]
    defaults: Optional[tuple[Any, ...]]
    ...


def getargspec(func: Callable[..., Any], *, include_kwonlyargs: bool = ...) -> ArgSpecFake:
    """Compat shim for getargspec deprecated in python 3.

    The main difference from inspect.getargspec (and inspect.getfullargspec
    for that matter) is that by using inspect.signature we are providing
    correct args/defaults for functools.wraps'ed functions.

    `include_kwonlyargs` option was added to centralize getting all args,
    even the ones which are kwonly (follow the ``*,``).

    For internal use and not advised for use in 3rd party code.
    Please use inspect.signature directly.
    """
    ...

_SIG_P = inspect.Parameter
_SIG_KIND_SELECTORS: dict[str, set[int]] = ...
@lru_cache()
def get_sig_param_names(f: Callable[..., Any], kinds: tuple[str, ...]) -> tuple[list[str], ...]:
    """A helper to selectively return parameters from inspect.signature.

    inspect.signature is the ultimate way for introspecting callables.  But
    its interface is not so convenient for a quick selection of parameters
    (AKA arguments) of desired type or combinations of such.  This helper
    should make it easier to retrieve desired collections of parameters.

    Since often it is desired to get information about multiple specific types
    of parameters, `kinds` is a list, so in a single invocation of `signature`
    and looping through the results we can obtain all information.

    Parameters
    ----------
    f: callable
    kinds: tuple with values from {'pos_any', 'pos_only', 'kw_any', 'kw_only', 'any'}
      Is a list of what kinds of args to return in result (tuple). Each element
      should be one of: 'any_pos' - positional or keyword which could be used
      positionally. 'kw_only' - keyword only (cannot be used positionally) arguments,
      'any_kw` - any keyword (could be a positional which could be used as a keyword),
      `any` -- any type from the above.

    Returns
    -------
    tuple:
      Each element is a list of parameters (names only) of that "kind".
    """
    ...

def any_re_search(regexes: str | list[str], value: str) -> bool:
    """Return if any of regexes (list or str) searches successfully for value"""
    ...

def not_supported_on_windows(msg: Optional[str] = ...) -> None:
    """A little helper to be invoked to consistently fail whenever functionality is
    not supported (yet) on Windows
    """
    ...

def get_home_envvars(new_home: str | Path) -> dict[str, str]:
    """Return dict with env variables to be adjusted for a new HOME

    Only variables found in current os.environ are adjusted.

    Parameters
    ----------
    new_home: str or Path
      New home path, in native to OS "schema"
    """
    ...

def is_interactive() -> bool:
    """Return True if all in/outs are open and tty.

    Note that in a somewhat abnormal case where e.g. stdin is explicitly
    closed, and any operation on it would raise a
    `ValueError("I/O operation on closed file")` exception, this function
    would just return False, since the session cannot be used interactively.
    """
    ...

def get_ipython_shell() -> Optional[Any]:
    """Detect if running within IPython and returns its `ip` (shell) object

    Returns None if not under ipython (no `get_ipython` function)
    """
    ...

def md5sum(filename: str | Path) -> str:
    """Compute an MD5 sum for the given file
    """
    ...

_encoded_dirsep = ...
_VCS_REGEX = ...
_DATALAD_REGEX = ...
def find_files(regex: str, topdir: str | Path = ..., exclude: Optional[str] = ..., exclude_vcs: bool = ..., exclude_datalad: bool = ..., dirs: bool = ...) -> Iterator[str]:
    """Generator to find files matching regex

    Parameters
    ----------
    regex: string
    exclude: string, optional
      Matches to exclude
    exclude_vcs:
      If True, excludes commonly known VCS subdirectories.  If string, used
      as regex to exclude those files (regex: `%r`)
    exclude_datalad:
      If True, excludes files known to be datalad meta-data files (e.g. under
      .datalad/ subdirectory) (regex: `%r`)
    topdir: string, optional
      Directory where to search
    dirs: bool, optional
      Whether to match directories as well as files
    """
    ...

def expandpath(path: str | Path, force_absolute: bool = ...) -> str:
    """Expand all variables and user handles in a path.

    By default return an absolute path
    """
    ...

def posix_relpath(path: str | Path, start: Optional[str | Path] = ...) -> str:
    """Behave like os.path.relpath, but always return POSIX paths...

    on any platform."""
    ...

def is_explicit_path(path: str | Path) -> bool:
    """Return whether a path explicitly points to a location

    Any absolute path, or relative path starting with either '../' or
    './' is assumed to indicate a location on the filesystem. Any other
    path format is not considered explicit."""
    ...

def rotree(path: str | Path, ro: bool = ..., chmod_files: bool = ...) -> None:
    """To make tree read-only or writable

    Parameters
    ----------
    path : string
      Path to the tree/directory to chmod
    ro : bool, optional
      Whether to make it R/O (default) or RW
    chmod_files : bool, optional
      Whether to operate also on files (not just directories)
    """
    ...

def rmtree(path: str | Path, chmod_files: bool | Literal[auto] = ..., children_only: bool = ..., *args: Any, **kwargs: Any) -> None:
    """To remove git-annex .git it is needed to make all files and directories writable again first

    Parameters
    ----------
    path: Path or str
       Path to remove
    chmod_files : string or bool, optional
       Whether to make files writable also before removal.  Usually it is just
       a matter of directories to have write permissions.
       If 'auto' it would chmod files on windows by default
    children_only : bool, optional
       If set, all files and subdirectories would be removed while the path
       itself (must be a directory) would be preserved
    `*args` :
    `**kwargs` :
       Passed into shutil.rmtree call
    """
    ...

def rmdir(path: str | Path, *args: Any, **kwargs: Any) -> None:
    """os.rmdir with our optional checking for open files"""
    ...

def get_open_files(path: str | Path, log_open: int = ...) -> dict[str, Any]:
    """Get open files under a path

    Note: This function is very slow on Windows.

    Parameters
    ----------
    path : str
      File or directory to check for open files under
    log_open : bool or int
      If set - logger level to use

    Returns
    -------
    dict
      path : pid

    """
    ...

_assert_no_open_files_cfg = ...
if _assert_no_open_files_cfg:
    def assert_no_open_files(path: str | Path) -> None:
        ...
    
else:
    def assert_no_open_files(path: str | Path) -> None:
        ...
    
def rmtemp(f: str | Path, *args: Any, **kwargs: Any) -> None:
    """Wrapper to centralize removing of temp files so we could keep them around

    It will not remove the temporary file/directory if DATALAD_TESTS_TEMP_KEEP
    environment variable is defined
    """
    ...

@overload
def file_basename(name: str | Path, return_ext: Literal[True]) -> tuple[str, str]:
    ...

@overload
def file_basename(name: str | Path, return_ext: Literal[False] = ...) -> str:
    ...

def file_basename(name: str | Path, return_ext: bool = ...) -> str | tuple[str, str]:
    """
    Strips up to 2 extensions of length up to 4 characters and starting with alpha
    not a digit, so we could get rid of .tar.gz etc
    """
    ...

def escape_filename(filename: str) -> str:
    """Surround filename in "" and escape " in the filename
    """
    ...

def encode_filename(filename: str | bytes) -> bytes:
    """Encode unicode filename
    """
    ...

def decode_input(s: str | bytes) -> str:
    """Given input string/bytes, decode according to stdin codepage (or UTF-8)
    if not defined

    If fails -- issue warning and decode allowing for errors
    being replaced
    """
    ...

if on_windows:
    def lmtime(filepath: str | Path, mtime: int | float) -> None:
        """Set mtime for files.  On Windows a merely adapter to os.utime
        """
        ...
    
else:
    def lmtime(filepath: str | Path, mtime: int | float) -> None:
        """Set mtime for files, while not de-referencing symlinks.

        To overcome absence of os.lutime

        Works only on linux and OSX ATM
        """
        ...
    
def ensure_tuple_or_list(obj: Any) -> list | tuple:
    """Given an object, wrap into a tuple if not list or tuple
    """
    ...

ListOrSet = TypeVar("ListOrSet", list, set)
def ensure_iter(s: Any, cls: type[ListOrSet], copy: bool = ..., iterate: bool = ...) -> ListOrSet:
    """Given not a list, would place it into a list. If None - empty list is returned

    Parameters
    ----------
    s: list or anything
    cls: class
      Which iterable class to ensure
    copy: bool, optional
      If correct iterable is passed, it would generate its shallow copy
    iterate: bool, optional
      If it is not a list, but something iterable (but not a str)
      iterate over it.
    """
    ...

def ensure_list(s: Any, copy: bool = ..., iterate: bool = ...) -> list:
    """Given not a list, would place it into a list. If None - empty list is returned

    Parameters
    ----------
    s: list or anything
    copy: bool, optional
      If list is passed, it would generate a shallow copy of the list
    iterate: bool, optional
      If it is not a list, but something iterable (but not a str)
      iterate over it.
    """
    ...

def ensure_result_list(r: Any) -> list:
    """Return a list of result records

    Largely same as ensure_list, but special casing a single dict being passed
    in, which a plain `ensure_list` would iterate over. Hence, this deals with
    the three ways datalad commands return results:
    - single dict
    - list of dicts
    - generator

    Used for result assertion helpers.
    """
    ...

@overload
def ensure_list_from_str(s: str, sep: str = ...) -> Optional[list[str]]:
    ...

@overload
def ensure_list_from_str(s: list[T], sep: str = ...) -> Optional[list[T]]:
    ...

def ensure_list_from_str(s: str | list[T], sep: str = ...) -> Optional[list[str]] | Optional[list[T]]:
    """Given a multiline string convert it to a list of return None if empty

    Parameters
    ----------
    s: str or list
    """
    ...

@overload
def ensure_dict_from_str(s: str, sep: str = ...) -> Optional[dict[str, str]]:
    ...

@overload
def ensure_dict_from_str(s: dict[K, V], sep: str = ...) -> Optional[dict[K, V]]:
    ...

def ensure_dict_from_str(s: str | dict[K, V], sep: str = ...) -> Optional[dict[str, str]] | Optional[dict[K, V]]:
    """Given a multiline string with key=value items convert it to a dictionary

    Parameters
    ----------
    s: str or dict

    Returns None if input s is empty
    """
    ...

def ensure_bytes(s: str | bytes, encoding: str = ...) -> bytes:
    """Convert/encode unicode string to bytes.

    If `s` isn't a string, return it as is.

    Parameters
    ----------
    encoding: str, optional
      Encoding to use.  "utf-8" is the default
    """
    ...

def ensure_unicode(s: str | bytes, encoding: Optional[str] = ..., confidence: Optional[float] = ...) -> str:
    """Convert/decode bytestring to unicode.

    If `s` isn't a bytestring, return it as is.

    Parameters
    ----------
    encoding: str, optional
      Encoding to use.  If None, "utf-8" is tried, and then if not a valid
      UTF-8, encoding will be guessed
    confidence: float, optional
      A value between 0 and 1, so if guessing of encoding is of lower than
      specified confidence, ValueError is raised
    """
    ...

def ensure_bool(s: Any) -> bool:
    """Convert value into boolean following convention for strings

    to recognize on,True,yes as True, off,False,no as False
    """
    ...

def unique(seq: Sequence[T], key: Optional[Callable[[T], Any]] = ..., reverse: bool = ...) -> list[T]:
    """Given a sequence return a list only with unique elements while maintaining order

    This is the fastest solution.  See
    https://www.peterbe.com/plog/uniqifiers-benchmark
    and
    http://stackoverflow.com/a/480227/1265472
    for more information.
    Enhancement -- added ability to compare for uniqueness using a key function

    Parameters
    ----------
    seq:
      Sequence to analyze
    key: callable, optional
      Function to call on each element so we could decide not on a full
      element, but on its member etc
    reverse: bool, optional
      If True, uniqueness checked in the reverse order, so that the later ones
      will take the order
    """
    ...

def map_items(func, v):
    """A helper to apply `func` to all elements (keys and values) within dict

    No type checking of values passed to func is done, so `func`
    should be resilient to values which it should not handle

    Initial usecase - apply_recursive(url_fragment, ensure_unicode)
    """
    ...

def partition(items: Iterable[T], predicate: Callable[[T], Any] = ...) -> tuple[Iterator[T], Iterator[T]]:
    """Partition `items` by `predicate`.

    Parameters
    ----------
    items : iterable
    predicate : callable
        A function that will be mapped over each element in `items`. The
        elements will partitioned based on whether the return value is false or
        true.

    Returns
    -------
    A tuple with two generators, the first for 'false' items and the second for
    'true' ones.

    Notes
    -----
    Taken from Peter Otten's snippet posted at
    https://nedbatchelder.com/blog/201306/filter_a_list_into_two_parts.html
    """
    ...

def generate_chunks(container: list[T], size: int) -> Iterator[list[T]]:
    """Given a container, generate chunks from it with size up to `size`
    """
    ...

def generate_file_chunks(files: list[str], cmd: str | list[str] | None = ...) -> Iterator[list[str]]:
    """Given a list of files, generate chunks of them to avoid exceeding cmdline length

    Parameters
    ----------
    files: list of str
    cmd: str or list of str, optional
      Command to account for as well
    """
    ...

def saved_generator(gen: Iterable[T]) -> tuple[Iterator[T], Iterator[T]]:
    """Given a generator returns two generators, where 2nd one just replays

    So the first one would be going through the generated items and 2nd one
    would be yielding saved items
    """
    ...

better_wraps = ...
def optional_args(decorator): # -> _Wrapped[Callable[..., Any], Any, Callable[..., Any], Any | Callable[..., Any]]:
    """allows a decorator to take optional positional and keyword arguments.
        Assumes that taking a single, callable, positional argument means that
        it is decorating a function, i.e. something like this::

            @my_decorator
            def function(): pass

        Calls decorator with decorator(f, `*args`, `**kwargs`)"""
    ...

def get_tempfile_kwargs(tkwargs: Optional[dict[str, Any]] = ..., prefix: str = ..., wrapped: Optional[Callable] = ...) -> dict[str, Any]:
    """Updates kwargs to be passed to tempfile. calls depending on env vars
    """
    ...

def line_profile(func: Callable[P, T]) -> Callable[P, T]:
    """Q&D helper to line profile the function and spit out stats
    """
    ...

@optional_args
def collect_method_callstats(func: Callable[P, T]) -> Callable[P, T]:
    """Figure out methods which call the method repeatedly on the same instance

    Use case(s):
      - .repo is expensive since does all kinds of checks.
      - .config is expensive transitively since it calls .repo each time

    TODO:
      - fancy one could look through the stack for the same id(self) to see if
        that location is already in memo.  That would hint to the cases where object
        is not passed into underlying functions, causing them to redo the same work
        over and over again
      - ATM might flood with all "1 lines" calls which are not that informative.
        The underlying possibly suboptimal use might be coming from their callers.
        It might or not relate to the previous TODO
    """
    ...

def never_fail(f: Callable[P, T]) -> Callable[P, Optional[T]]:
    """Assure that function never fails -- all exceptions are caught

    Returns `None` if function fails internally.
    """
    ...

def shortened_repr(value: Any, l: int = ...) -> str:
    ...

def __auto_repr__(obj: Any, short: bool = ...) -> str:
    ...

@optional_args
def auto_repr(cls: type[T], short: bool = ...) -> type[T]:
    """Decorator for a class to assign it an automagic quick and dirty __repr__

    It uses public class attributes to prepare repr of a class

    Original idea: http://stackoverflow.com/a/27799004/1265472
    """
    ...

def todo_interface_for_extensions(f: T) -> T:
    ...

@contextmanager
def nothing_cm() -> Iterator[None]:
    """Just a dummy cm to programmically switch context managers"""
    ...

class SwallowOutputsAdapter:
    """Little adapter to help getting out/err values
    """
    def __init__(self) -> None:
        ...
    
    @property
    def out(self) -> str:
        ...
    
    @property
    def err(self) -> str:
        ...
    
    @property
    def handles(self) -> tuple[TextIO, TextIO]:
        ...
    
    def cleanup(self) -> None:
        ...
    


@contextmanager
def swallow_outputs() -> Iterator[SwallowOutputsAdapter]:
    """Context manager to help consuming both stdout and stderr, and print()

    stdout is available as cm.out and stderr as cm.err whenever cm is the
    yielded context manager.
    Internally uses temporary files to guarantee absent side-effects of swallowing
    into StringIO which lacks .fileno.

    print mocking is necessary for some uses where sys.stdout was already bound
    to original sys.stdout, thus mocking it later had no effect. Overriding
    print function had desired effect
    """
    ...

class SwallowLogsAdapter:
    """Little adapter to help getting out values

    And to stay consistent with how swallow_outputs behaves
    """
    def __init__(self, file_: str | Path | None) -> None:
        ...
    
    @property
    def out(self) -> str:
        ...
    
    @property
    def lines(self) -> list[str]:
        ...
    
    @property
    def handle(self) -> IO[str]:
        ...
    
    def cleanup(self) -> None:
        ...
    
    def assert_logged(self, msg: Optional[str] = ..., level: Optional[str] = ..., regex: bool = ..., **kwargs: Any) -> None:
        """Provide assertion on whether a msg was logged at a given level

        If neither `msg` nor `level` provided, checks if anything was logged
        at all.

        Parameters
        ----------
        msg: str, optional
          Message (as a regular expression, if `regex`) to be searched.
          If no msg provided, checks if anything was logged at a given level.
        level: str, optional
          String representing the level to be logged
        regex: bool, optional
          If False, regular `assert_in` is used
        **kwargs: str, optional
          Passed to `assert_re_in` or `assert_in`
        """
        ...
    


@contextmanager
def swallow_logs(new_level: str | int | None = ..., file_: str | Path | None = ..., name: str = ...) -> Iterator[SwallowLogsAdapter]:
    """Context manager to consume all logs."""
    ...

@contextmanager
def disable_logger(logger: Optional[logging.Logger] = ...) -> Iterator[logging.Logger]:
    """context manager to temporarily disable logging

    This is to provide one of swallow_logs' purposes without unnecessarily
    creating temp files (see gh-1865)

    Parameters
    ----------
    logger: Logger
        Logger whose handlers will be ordered to not log anything.
        Default: datalad's topmost Logger ('datalad')
    """
    class NullFilter(logging.Filter):
        """Filter class to reject all records
        """
        ...
    
    

@contextmanager
def lock_if_required(lock_required: bool, lock: threading.Lock) -> Iterator[threading.Lock]:
    """ Acquired and released the provided lock if indicated by a flag"""
    ...

def ensure_dir(*args: str) -> str:
    """Make sure directory exists.

    Joins the list of arguments to an os-specific path to the desired
    directory and creates it, if it not exists yet.
    """
    ...

def updated(d: dict[K, V], update: dict[K, V]) -> dict[K, V]:
    """Return a copy of the input with the 'update'

    Primarily for updating dictionaries
    """
    ...

_pwd_mode: Optional[str] = ...
def getpwd() -> str:
    """Try to return a CWD without dereferencing possible symlinks

    This function will try to use PWD environment variable to provide a current
    working directory, possibly with some directories along the path being
    symlinks to other directories.  Unfortunately, PWD is used/set only by the
    shell and such functions as `os.chdir` and `os.getcwd` nohow use or modify
    it, thus `os.getcwd()` returns path with links dereferenced.

    While returning current working directory based on PWD env variable we
    verify that the directory is the same as `os.getcwd()` after resolving all
    symlinks.  If that verification fails, we fall back to always use
    `os.getcwd()`.

    Initial decision to either use PWD env variable or os.getcwd() is done upon
    the first call of this function.
    """
    ...

class chpwd:
    """Wrapper around os.chdir which also adjusts environ['PWD']

    The reason is that otherwise PWD is simply inherited from the shell
    and we have no ability to assess directory path without dereferencing
    symlinks.

    If used as a context manager it allows to temporarily change directory
    to the given path
    """
    def __init__(self, path: str | Path | None, mkdir: bool = ..., logsuffix: str = ...) -> None:
        ...
    
    def __enter__(self) -> None:
        ...
    
    def __exit__(self, exc_type: Optional[type[BaseException]], exc_val: Optional[BaseException], exc_tb: Optional[TracebackType]) -> None:
        ...
    


def dlabspath(path: str | Path, norm: bool = ...) -> str:
    """Symlinks-in-the-cwd aware abspath

    os.path.abspath relies on os.getcwd() which would not know about symlinks
    in the path

    TODO: we might want to norm=True by default to match behavior of
    os .path.abspath?
    """
    ...

def with_pathsep(path: str) -> str:
    """Little helper to guarantee that path ends with /"""
    ...

def get_path_prefix(path: str | Path, pwd: Optional[str] = ...) -> str:
    """Get path prefix (for current directory)

    Returns relative path to the topdir, if we are under topdir, and if not
    absolute path to topdir.  If `pwd` is not specified - current directory
    assumed
    """
    ...

def path_startswith(path: str, prefix: str) -> bool:
    """Return True if path starts with prefix path

    Parameters
    ----------
    path: str
    prefix: str
    """
    ...

def path_is_subpath(path: str, prefix: str) -> bool:
    """Return True if path is a subpath of prefix

    It will return False if path == prefix.

    Parameters
    ----------
    path: str
    prefix: str
    """
    ...

def knows_annex(path: str | Path) -> bool:
    """Returns whether at a given path there is information about an annex

    It is just a thin wrapper around GitRepo.is_with_annex() classmethod
    which also checks for `path` to exist first.

    This includes actually present annexes, but also uninitialized ones, or
    even the presence of a remote annex branch.
    """
    ...

@contextmanager
def make_tempfile(content: str | bytes | None = ..., wrapped: Optional[Callable[..., Any]] = ..., **tkwargs: Any) -> Iterator[str]:
    """Helper class to provide a temporary file name and remove it at the end (context manager)

    Parameters
    ----------
    mkdir : bool, optional (default: False)
        If True, temporary directory created using tempfile.mkdtemp()
    content : str or bytes, optional
        Content to be stored in the file created
    wrapped : function, optional
        If set, function name used to prefix temporary file name
    `**tkwargs`:
        All other arguments are passed into the call to tempfile.mk{,d}temp(),
        and resultant temporary filename is passed as the first argument into
        the function t.  If no 'prefix' argument is provided, it will be
        constructed using module and function names ('.' replaced with
        '_').

    To change the used directory without providing keyword argument 'dir' set
    DATALAD_TESTS_TEMP_DIR.

    Examples
    --------
        >>> from os.path import exists
        >>> from datalad.utils import make_tempfile
        >>> with make_tempfile() as fname:
        ...    k = open(fname, 'w').write('silly test')
        >>> assert not exists(fname)  # was removed

        >>> with make_tempfile(content="blah") as fname:
        ...    assert open(fname).read() == "blah"
    """
    ...

def get_timestamp_suffix(time_: int | time.struct_time | None = ..., prefix: str = ...) -> str:
    """Return a time stamp (full date and time up to second)

    primarily to be used for generation of log files names
    """
    ...

def get_logfilename(dspath: str | Path, cmd: str = ...) -> str:
    """Return a filename to use for logging under a dataset/repository

    directory would be created if doesn't exist, but dspath must exist
    and be a directory
    """
    ...

def get_trace(edges: Sequence[tuple[T, T]], start: T, end: T, trace: Optional[list[T]] = ...) -> Optional[list[T]]:
    """Return the trace/path to reach a node in a tree.

    Parameters
    ----------
    edges : sequence(2-tuple)
      The tree given by a sequence of edges (parent, child) tuples. The
      nodes can be identified by any value and data type that supports
      the '==' operation.
    start :
      Identifier of the start node. Must be present as a value in the parent
      location of an edge tuple in order to be found.
    end :
      Identifier of the target/end node. Must be present as a value in the child
      location of an edge tuple in order to be found.
    trace : list
      Mostly useful for recursive calls, and used internally.

    Returns
    -------
    None or list
      Returns a list with the trace to the target (the starts and the target
      are not included in the trace, hence if start and end are directly connected
      an empty list is returned), or None when no trace to the target can be found,
      or start and end are identical.
    """
    ...

def get_dataset_root(path: str | Path) -> Optional[str]:
    """Return the root of an existent dataset containing a given path

    The root path is returned in the same absolute or relative form
    as the input argument. If no associated dataset exists, or the
    input path doesn't exist, None is returned.

    If `path` is a symlink or something other than a directory, its
    the root dataset containing its parent directory will be reported.
    If none can be found, at a symlink at `path` is pointing to a
    dataset, `path` itself will be reported as the root.

    Parameters
    ----------
    path : Path-like

    Returns
    -------
    str or None
    """
    ...

def try_multiple(ntrials: int, exception: type[BaseException], base: float, f: Callable[P, T], *args: P.args, **kwargs: P.kwargs) -> T:
    """Call f multiple times making exponentially growing delay between the calls"""
    ...

@optional_args
def try_multiple_dec(f: Callable[P, T], ntrials: Optional[int] = ..., duration: float = ..., exceptions: type[BaseException] | tuple[type[BaseException], ...] | None = ..., increment_type: Literal[exponential] | None = ..., exceptions_filter: Optional[Callable[[BaseException], Any]] = ..., logger: Optional[Callable] = ...) -> Callable[P, T]:
    """Decorator to try function multiple times.

    Main purpose is to decorate functions dealing with removal of files/directories
    and which might need a few seconds to work correctly on Windows which takes
    its time to release files/directories.

    Parameters
    ----------
    ntrials: int, optional
    duration: float, optional
      Seconds to sleep before retrying.
    increment_type: {None, 'exponential'}
      Note that if it is exponential, duration should typically be > 1.0
      so it grows with higher power
    exceptions: Exception or tuple of Exceptions, optional
      Exception or a tuple of multiple exceptions, on which to retry
    exceptions_filter: callable, optional
      If provided, this function will be called with a caught exception
      instance.  If function returns True - we will re-try, if False - exception
      will be re-raised without retrying.
    logger: callable, optional
      Logger to log upon failure.  If not provided, will use stock logger
      at the level of 5 (heavy debug).
    """
    ...

@try_multiple_dec
def unlink(f: str | Path) -> None:
    """'Robust' unlink.  Would try multiple times

    On windows boxes there is evidence for a latency of more than a second
    until a file is considered no longer "in-use".
    WindowsError is not known on Linux, and if IOError or any other
    exception
    is thrown then if except statement has WindowsError in it -- NameError
    also see gh-2533
    """
    ...

def slash_join(base: Optional[str], extension: Optional[str]) -> Optional[str]:
    """Join two strings with a '/', avoiding duplicate slashes

    If any of the strings is None the other is returned as is.
    """
    ...

def open_r_encdetect(fname: str | Path, readahead: int = ...) -> IO[str]:
    """Return a file object in read mode with auto-detected encoding

    This is helpful when dealing with files of unknown encoding.

    Parameters
    ----------
    readahead: int, optional
      How many bytes to read for guessing the encoding type.  If
      negative - full file will be read
    """
    ...

@overload
def read_file(fname: str | Path, decode: Literal[True] = ...) -> str:
    ...

@overload
def read_file(fname: str | Path, decode: Literal[False]) -> bytes:
    ...

def read_file(fname: str | Path, decode: Literal[True, False] = ...) -> str | bytes:
    """A helper to read file passing content via ensure_unicode

    Parameters
    ----------
    decode: bool, optional
      if False, no ensure_unicode and file content returned as bytes
    """
    ...

def read_csv_lines(fname: str | Path, dialect: Optional[str] = ..., readahead: int = ..., **kwargs: Any) -> Iterator[dict[str, str]]:
    """A generator of dict records from a CSV/TSV

    Automatically guesses the encoding for each record to convert to UTF-8

    Parameters
    ----------
    fname: str
      Filename
    dialect: str, optional
      Dialect to specify to csv.reader. If not specified -- guessed from
      the file, if fails to guess, "excel-tab" is assumed
    readahead: int, optional
      How many bytes to read from the file to guess the type
    **kwargs
      Passed to `csv.reader`
    """
    ...

def import_modules(modnames: Iterable[str], pkg: str, msg: str = ..., log: Callable[[str], Any] = ...) -> list[ModuleType]:
    """Helper to import a list of modules without failing if N/A

    Parameters
    ----------
    modnames: list of str
      List of module names to import
    pkg: str
      Package under which to import
    msg: str, optional
      Message template for .format() to log at DEBUG level if import fails.
      Keys {module} and {package} will be provided and ': {exception}' appended
    log: callable, optional
      Logger call to use for logging messages
    """
    ...

def import_module_from_file(modpath: str, pkg: Optional[ModuleType] = ..., log: Callable[[str], Any] = ...) -> ModuleType:
    """Import provided module given a path

    TODO:
    - RF/make use of it in pipeline.py which has similar logic
    - join with import_modules above?

    Parameters
    ----------
    pkg: module, optional
       If provided, and modpath is under pkg.__path__, relative import will be
       used
    """
    ...

def get_encoding_info() -> dict[str, str]:
    """Return a dictionary with various encoding/locale information"""
    ...

def get_envvars_info() -> dict[str, str]:
    ...

class SequenceFormatter(string.Formatter):
    """string.Formatter subclass with special behavior for sequences.

    This class delegates formatting of individual elements to another
    formatter object. Non-list objects are formatted by calling the
    delegate formatter's "format_field" method. List-like objects
    (list, tuple, set, frozenset) are formatted by formatting each
    element of the list according to the specified format spec using
    the delegate formatter and then joining the resulting strings with
    a separator (space by default).
    """
    def __init__(self, separator: str = ..., element_formatter: string.Formatter = ..., *args: Any, **kwargs: Any) -> None:
        ...
    
    def format_element(self, elem: Any, format_spec: str) -> Any:
        """Format a single element

        For sequences, this is called once for each element in a
        sequence. For anything else, it is called on the entire
        object. It is intended to be overridden in subclases.
        """
        ...
    
    def format_field(self, value: Any, format_spec: str) -> Any:
        ...
    


class File:
    """Helper for a file entry in the create_tree/@with_tree

    It allows to define additional settings for entries
    """
    def __init__(self, name: str, executable: bool = ...) -> None:
        """

        Parameters
        ----------
        name : str
          Name of the file
        executable: bool, optional
          Make it executable
        """
        ...
    
    def __str__(self) -> str:
        ...
    


TreeSpec = Union[Tuple[Tuple[Union[str, File], "Load"], ...], List[Tuple[Union[str, File], "Load"]], Dict[Union[str, File], "Load"],]
Load = Union[str, bytes, "TreeSpec"]
def create_tree_archive(path: str, name: str, load: TreeSpec, overwrite: bool = ..., archives_leading_dir: bool = ...) -> None:
    """Given an archive `name`, create under `path` with specified `load` tree
    """
    ...

def create_tree(path: str, tree: TreeSpec, archives_leading_dir: bool = ..., remove_existing: bool = ...) -> None:
    """Given a list of tuples (name, load) create such a tree

    if load is a tuple itself -- that would create either a subtree or an archive
    with that content and place it into the tree if name ends with .tar.gz
    """
    ...

def get_suggestions_msg(values: Optional[str | Iterable[str]], known: str, sep: str = ...) -> str:
    """Return a formatted string with suggestions for values given the known ones
    """
    ...

def bytes2human(n: int | float, format: str = ...) -> str:
    """
    Convert n bytes into a human readable string based on format.
    symbols can be either "customary", "customary_ext", "iec" or "iec_ext",
    see: http://goo.gl/kTQMs

      >>> from datalad.utils import bytes2human
      >>> bytes2human(1)
      '1.0 B'
      >>> bytes2human(1024)
      '1.0 KB'
      >>> bytes2human(1048576)
      '1.0 MB'
      >>> bytes2human(1099511627776127398123789121)
      '909.5 YB'

      >>> bytes2human(10000, "%(value).1f %(symbol)s/sec")
      '9.8 K/sec'

      >>> # precision can be adjusted by playing with %f operator
      >>> bytes2human(10000, format="%(value).5f %(symbol)s")
      '9.76562 K'

    Taken from: http://goo.gl/kTQMs and subsequently simplified
    Original Author: Giampaolo Rodola' <g.rodola [AT] gmail [DOT] com>
    License: MIT
    """
    ...

def quote_cmdlinearg(arg: str) -> str:
    """Perform platform-appropriate argument quoting"""
    ...

def guard_for_format(arg: str) -> str:
    """Replace { and } with {{ and }}

    To be used in cases if arg is not expected to have provided
    by user .format() placeholders, but 'arg' might become a part
    of a composite passed to .format(), e.g. via 'Run'
    """
    ...

def join_cmdline(args: Iterable[str]) -> str:
    """Join command line args into a string using quote_cmdlinearg
    """
    ...

def split_cmdline(s: str) -> list[str]:
    """Perform platform-appropriate command line splitting.

    Identical to `shlex.split()` on non-windows platforms.

    Modified from https://stackoverflow.com/a/35900070
    """
    ...

def get_wrapped_class(wrapped: Callable) -> type:
    """Determine the command class a wrapped __call__ belongs to"""
    ...

assure_tuple_or_list = ...
assure_iter = ...
assure_list = ...
assure_list_from_str = ...
assure_dict_from_str = ...
assure_bytes = ...
assure_unicode = ...
assure_bool = ...
assure_dir = ...
def check_symlink_capability(path: Path, target: Path) -> bool:
    """helper similar to datalad.tests.utils_pytest.has_symlink_capability

    However, for use in a datalad command context, we shouldn't
    assume to be able to write to tmpfile and also not import a whole lot from
    datalad's test machinery. Finally, we want to know, whether we can create a
    symlink at a specific location, not just somewhere. Therefore use
    arbitrary path to test-build a symlink and delete afterwards. Suitable
    location can therefore be determined by high lever code.

    Parameters
    ----------
    path: Path
    target: Path

    Returns
    -------
    bool
    """
    ...

def obtain_write_permission(path: Path) -> Optional[int]:
    """Obtains write permission for `path` and returns previous mode if a
    change was actually made.

    Parameters
    ----------
    path: Path
      path to try to obtain write permission for

    Returns
    -------
    int or None
      previous mode of `path` as return by stat().st_mode if a change in
      permission was actually necessary, `None` otherwise.
    """
    ...

@contextmanager
def ensure_write_permission(path: Path) -> Iterator[None]:
    """Context manager to get write permission on `path` and
    restore original mode afterwards.

    Parameters
    ----------
    path: Path
      path to the target file

    Raises
    ------
    PermissionError
       if write permission could not be obtained
    """
    ...

