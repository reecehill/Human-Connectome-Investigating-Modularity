"""
This type stub file was generated by pyright.
"""

"""Utility routines for workflow graphs"""
logger = ...
def save_hashfile(hashfile, hashed_inputs): # -> None:
    """Store a hashfile"""
    ...

def nodelist_runner(nodes, updatehash=..., stop_first=...): # -> Generator[tuple[Unknown, Unknown | None, LiteralString | None], None, None]:
    """
    A generator that iterates and over a list of ``nodes`` and
    executes them.

    """
    ...

def write_node_report(node, result=..., is_mapnode=...): # -> None:
    """Write a report file for a node."""
    ...

def write_report(node, report_type=..., is_mapnode=...): # -> None:
    """Write a report file for a node - DEPRECATED"""
    ...

def save_resultfile(result, cwd, name, rebase=...): # -> None:
    """Save a result pklz file to ``cwd``."""
    ...

def load_resultfile(results_file, resolve=...): # -> Any:
    """
    Load InterfaceResult file from path.

    Parameters
    ----------
    results_file : pathlike
        Path to an existing pickle (``result_<interface name>.pklz``) created with
        ``save_resultfile``.
        Raises ``FileNotFoundError`` if ``results_file`` does not exist.
    resolve : bool
        Determines whether relative paths will be resolved to absolute (default is ``True``).

    Returns
    -------
    result : InterfaceResult
        A Nipype object containing the runtime, inputs, outputs and other interface information
        such as a traceback in the case of errors.

    """
    ...

def strip_temp(files, wd): # -> list[Unknown]:
    """Remove temp from a list of file paths"""
    ...

def format_node(node, format=..., include_config=...): # -> list[str | Unknown]:
    """Format a node in a given output syntax."""
    ...

def modify_paths(object, relative=..., basedir=...): # -> tuple[Unknown, ...] | str | bytes | dict[Unknown, Unknown] | list[Unknown]:
    """Convert paths in data structure to either full paths or relative paths

    Supports combinations of lists, dicts, tuples, strs

    Parameters
    ----------

    relative : boolean indicating whether paths should be set relative to the
               current directory
    basedir : default os.getcwd()
              what base directory to use as default
    """
    ...

def get_print_name(node, simple_form=...): # -> LiteralString:
    """Get the name of the node

    For example, a node containing an instance of interfaces.fsl.BET
    would be called nodename.BET.fsl

    """
    ...

def expand_iterables(iterables, synchronize=...): # -> list[Unknown] | list[dict[Unknown, Unknown] | Unknown]:
    ...

def count_iterables(iterables, synchronize=...): # -> SupportsRichComparisonT@max | int:
    """Return the number of iterable expansion nodes.

    If synchronize is True, then the count is the maximum number
    of iterables value lists.
    Otherwise, the count is the product of the iterables value
    list sizes.
    """
    ...

def walk(children, level=..., path=..., usename=...): # -> Generator[Unknown | dict[Unknown, Unknown], None, None]:
    """Generate all the full paths in a tree, as a dict.

    Examples
    --------
    >>> from nipype.pipeline.engine.utils import walk
    >>> iterables = [('a', lambda: [1, 2]), ('b', lambda: [3, 4])]
    >>> [val['a'] for val in walk(iterables)]
    [1, 1, 2, 2]
    >>> [val['b'] for val in walk(iterables)]
    [3, 4, 3, 4]
    """
    ...

def synchronize_iterables(iterables): # -> list[Unknown]:
    """Synchronize the given iterables in item-wise order.

    Return: the {field: value} dictionary list

    Examples
    --------
    >>> from nipype.pipeline.engine.utils import synchronize_iterables
    >>> iterables = dict(a=lambda: [1, 2], b=lambda: [3, 4])
    >>> synced = synchronize_iterables(iterables)
    >>> synced == [{'a': 1, 'b': 3}, {'a': 2, 'b': 4}]
    True
    >>> iterables = dict(a=lambda: [1, 2], b=lambda: [3], c=lambda: [4, 5, 6])
    >>> synced = synchronize_iterables(iterables)
    >>> synced == [{'a': 1, 'b': 3, 'c': 4}, {'a': 2, 'c': 5}, {'c': 6}]
    True
    """
    ...

def evaluate_connect_function(function_source, args, first_arg):
    ...

def get_levels(G): # -> dict[Unknown, Unknown]:
    ...

def generate_expanded_graph(graph_in):
    """Generates an expanded graph based on node parameterization

    Parameterization is controlled using the `iterables` field of the
    pipeline elements.  Thus if there are two nodes with iterables a=[1,2]
    and b=[3,4] this procedure will generate a graph with sub-graphs
    parameterized as (a=1,b=3), (a=1,b=4), (a=2,b=3) and (a=2,b=4).
    """
    ...

def export_graph(graph_in, base_dir=..., show=..., use_execgraph=..., show_connectinfo=..., dotfilename=..., format=..., simple_form=...): # -> str:
    """Displays the graph layout of the pipeline

    This function requires that pygraphviz and matplotlib are available on
    the system.

    Parameters
    ----------

    show : boolean
    Indicate whether to generate pygraphviz output fromn
    networkx. default [False]

    use_execgraph : boolean
    Indicates whether to use the specification graph or the
    execution graph. default [False]

    show_connectioninfo : boolean
    Indicates whether to show the edge data on the graph. This
    makes the graph rather cluttered. default [False]
    """
    ...

def format_dot(dotfilename, format=...):
    """Dump a directed graph (Linux only; install via `brew` on OSX)"""
    ...

def get_all_files(infile): # -> list[Unknown]:
    ...

def walk_outputs(object): # -> list[tuple[str | bytes, Literal['f']]] | list[tuple[str | bytes, Literal['d']]]:
    """Extract every file and directory from a python structure"""
    ...

def walk_files(cwd): # -> Generator[Unknown, None, None]:
    ...

def clean_working_directory(outputs, cwd, inputs, needed_outputs, config, files2keep=..., dirs2keep=...): # -> None:
    """Removes all files not needed for further analysis from the directory"""
    ...

def merge_dict(d1, d2, merge=...): # -> dict[Unknown, Unknown]:
    """
    Merges two dictionaries, non-destructively, combining
    values on duplicate keys as defined by the optional merge
    function.  The default behavior replaces the values in d1
    with corresponding values in d2.  (There is no other generally
    applicable merge strategy, but often you'll have homogeneous
    types in your dicts, so specifying a merge technique can be
    valuable.)

    Examples:

    >>> d1 = {'a': 1, 'c': 3, 'b': 2}
    >>> d2 = merge_dict(d1, d1)
    >>> len(d2)
    3
    >>> [d2[k] for k in ['a', 'b', 'c']]
    [1, 2, 3]

    >>> d3 = merge_dict(d1, d1, lambda x,y: x+y)
    >>> len(d3)
    3
    >>> [d3[k] for k in ['a', 'b', 'c']]
    [2, 4, 6]

    """
    ...

def merge_bundles(g1, g2):
    ...

def write_workflow_prov(graph, filename=..., format=...): # -> ProvDocument:
    """Write W3C PROV Model JSON file"""
    ...

def write_workflow_resources(graph, filename=..., append=...): # -> str:
    """
    Generate a JSON file with profiling traces that can be loaded
    in a pandas DataFrame or processed with JavaScript like D3.js
    """
    ...

def topological_sort(graph, depth_first=...): # -> tuple[list[Unknown], None] | tuple[list[Unknown], list[Unknown]]:
    """Returns a depth first sorted order if depth_first is True"""
    ...

